#  FinOps Dashboard Multi-Cloud

##  Vue d'ensemble
**Nom** : FinOps Dashboard - Analyse des Co√ªts Cloud Multi-Compte (AWS + Azure)  
**Objectif** : Pipeline ETL automatis√© pour analyser, optimiser et visualiser les co√ªts cloud multi-providers  
**Valeur Business** : R√©duction des co√ªts cloud de 15-30% via identification des optimisations et anomalies  
**Niveau** : Projet professionnel production-ready  



##  Architecture Technique



##  Structure du projet
```
finops-dashboard/
‚îú‚îÄ‚îÄ venv/                    # Environnement Python
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                 # Donn√©es brutes
‚îÇ   ‚îî‚îÄ‚îÄ processed/           # Donn√©es transform√©es et KPIs
‚îú‚îÄ‚îÄ scripts/                 # Scripts ETL
‚îú‚îÄ‚îÄ airflow/                 # DAGs, logs, Docker
‚îú‚îÄ‚îÄ logs/                    # Logs pipeline
‚îú‚îÄ‚îÄ .env                     # Credentials (gitignored)
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ dashboard.py             # Streamlit dashboard
‚îî‚îÄ‚îÄ README.md                # Documentation compl√®te
```

## üîß Stack Technique

**Backend & ETL** : Python 3.11, pandas, boto3, azure-mgmt-costmanagement, Airflow 2.7.3, Docker  
**Stockage** : CSV, JSON, Amazon S3  
**Visualisation** : Streamlit, Plotly  
**Cloud Providers** : AWS (Cost Explorer, S3, IAM), Azure (Cost Management API, Service Principal)  

 

##  Workflow Pipeline ETL

1. **Extraction Multi-Cloud** (`extract_multicloud_costs.py`)  
   - AWS Cost Explorer + Azure Cost Management API  
   - Gestion erreurs : fallback avec donn√©es simul√©es ou placeholder  
   - Output : `data/raw/multicloud_costs_YYYYMMDD.csv`

2. **Transformation** (`transform_costs.py`)  
   - Nettoyage, enrichissement, agr√©gations  
   - Calcul KPIs et d√©tection anomalies  
   - Output : `data/processed/*.csv` et `kpis_*.json`

3. **Upload S3** (`s3_uploader.py`)  
   - Organisation S3 : `processed/`, `reports/`, `kpis/`  
   - R√©tention : suppression > 30 jours

4. **Notification / Logs**  
   - Logs d√©taill√©s : `logs/pipeline.log`  
   - Rapports texte : `logs/report_*.txt`  
   - Historique ex√©cutions : `logs/execution_history.json`

 

## Dashboard Streamlit

- KPI Cards : Co√ªt total, moyen/jour, tendance, anomalies  
- Graphiques interactifs : √©volution journali√®re, top services, par cat√©gorie, comptes, comparaison multi-cloud  
- Filtres dynamiques : p√©riode, cloud, compte, cat√©gorie  
- Export CSV des donn√©es filtr√©es et Top 10 services  

 

##  Configuration Cloud

**AWS** : Access Key + Secret, S3 Bucket `finops-dashboard-data`, IAM Role `finops_user`  
**Azure** : Service Principal `finops-cost-reader`, Cost Management Reader, Tenant/Client IDs + Secret  

 

##  Orchestration Airflow

- DAG principal : `finops_cost_analysis_pipeline` (ETL quotidien 8h00)  
- DAG secondaire : `finops_s3_cleanup` (suppression fichiers S3 >30j)  
- Docker Compose pour Airflow + PostgreSQL  
- Logs & monitoring via Airflow UI  

 

##  Donn√©es & KPIs

**Donn√©es brutes** : Date, Cloud, Service, Region, AccountName, AccountId, Cost, Currency  
**Donn√©es enrichies** : + dimensions temporelles, cat√©gories services, agr√©gations, KPIs  

 

##  Instructions pour lancer le projet

1. Cloner le repo  
2. Cr√©er `.env` avec credentials AWS/Azure  
3. Installer dependencies : `pip install -r airflow/requirements.txt`  
4. Lancer Airflow avec Docker Compose : `docker-compose up -d`  
5. Ex√©cuter pipeline : `python run_pipeline_now.py`  
6. Lancer dashboard : `streamlit run dashboard.py`  

 

