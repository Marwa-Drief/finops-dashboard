"""
DAG Airflow - FinOps Cost Analysis Pipeline
ETL complet : Extraction â†’ Transformation â†’ Export S3
"""

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.utils.dates import days_ago
from datetime import datetime, timedelta
import sys
import os

# Ajouter les scripts au path
sys.path.insert(0, '/opt/airflow/scripts')

from s3_uploader import S3Uploader

# Configuration du DAG
default_args = {
    'owner': 'finops-team',
    'depends_on_past': False,
    'email': ['votre-email@example.com'],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'finops_cost_analysis_pipeline',
    default_args=default_args,
    description='Pipeline ETL complet pour analyse des coÃ»ts cloud',
    schedule_interval='0 8 * * *',  # Tous les jours Ã  8h
    start_date=days_ago(1),
    catchup=False,
    tags=['finops', 'aws', 'costs', 'etl'],
)


def extract_costs(**context):
    """TÃ¢che d'extraction multi-cloud"""
    import subprocess
    
    print("ðŸŒ Extraction multi-cloud...")
    
    result = subprocess.run(
        ['python', '/opt/airflow/scripts/extract_multicloud_costs.py'],
        capture_output=True,
        text=True
    )
    
    if result.returncode != 0:
        raise Exception(f"Erreur extraction : {result.stderr}")
    
    print("âœ… Extraction multi-cloud terminÃ©e")
    return "extraction_success"


def transform_costs(**context):
    """TÃ¢che de transformation des donnÃ©es"""
    import subprocess
    
    print("ðŸ”„ Transformation des donnÃ©es...")
    
    result = subprocess.run(
        ['python', '/opt/airflow/scripts/transform_costs.py'],
        capture_output=True,
        text=True
    )
    
    if result.returncode != 0:
        raise Exception(f"Erreur transformation : {result.stderr}")
    
    print("âœ… Transformation terminÃ©e")
    return "transformation_success"


def upload_to_s3(**context):
    """TÃ¢che d'upload vers S3"""
    print("ðŸ“¤ Upload vers S3...")
    
    uploader = S3Uploader()
    count = uploader.upload_latest_data()
    
    if count == 0:
        raise Exception("Aucun fichier uploadÃ© vers S3")
    
    print(f"âœ… {count} fichiers uploadÃ©s vers S3")
    return f"uploaded_{count}_files"


def send_notification(**context):
    """Envoie une notification de succÃ¨s"""
    import json
    
    ti = context['task_instance']
    
    # RÃ©cupÃ©rer les rÃ©sultats des tÃ¢ches prÃ©cÃ©dentes
    extraction_result = ti.xcom_pull(task_ids='extract_costs_task')
    transformation_result = ti.xcom_pull(task_ids='transform_costs_task')
    upload_result = ti.xcom_pull(task_ids='upload_to_s3_task')
    
    print("="*60)
    print("ðŸ“Š PIPELINE FINOPS - RÃ‰SUMÃ‰ D'EXÃ‰CUTION")
    print("="*60)
    print(f"ðŸ“… Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"âœ… Extraction : {extraction_result}")
    print(f"âœ… Transformation : {transformation_result}")
    print(f"âœ… Upload S3 : {upload_result}")
    print("="*60)
    
    return "pipeline_complete"


# DÃ©finition des tÃ¢ches
task_extract = PythonOperator(
    task_id='extract_costs_task',
    python_callable=extract_costs,
    dag=dag,
)

task_transform = PythonOperator(
    task_id='transform_costs_task',
    python_callable=transform_costs,
    dag=dag,
)

task_upload_s3 = PythonOperator(
    task_id='upload_to_s3_task',
    python_callable=upload_to_s3,
    dag=dag,
)

task_notify = PythonOperator(
    task_id='send_notification_task',
    python_callable=send_notification,
    dag=dag,
)

# DÃ©finir les dÃ©pendances (ordre d'exÃ©cution)
task_extract >> task_transform >> task_upload_s3 >> task_notify